{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Landmark Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/anaconda3/envs/python36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda3/envs/python36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda3/envs/python36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda3/envs/python36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda3/envs/python36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda3/envs/python36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import cv2\n",
    "import dlib\n",
    "import tensorflow \n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's create a path to all the images using the os library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "global basedir, image_paths, target_size\n",
    "basedir = './datasets/celeba'\n",
    "images_dir = os.path.join(basedir,'img')\n",
    "labels_filename = 'labels2.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task we will use the Facial Landmark approchach. \"The aim is to find frontal human faces in an image and estimate their pose. The pose takes the form of 68 landmarks. These are points on the face such as the corners of the mouth, along the eyebrows, on the eyes, and so forth.\" (http://dlib.net/face_landmark_detection.py.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we will define three functions from label2_landmark.py as follow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_to_np(shape, dtype=\"int\"):\n",
    "    # initialize the list of (x, y)-coordinates\n",
    "    coords = np.zeros((shape.num_parts, 2), dtype=dtype)\n",
    "\n",
    "    # loop over all facial landmarks and convert them\n",
    "    # to a 2-tuple of (x, y)-coordinates\n",
    "    for i in range(0, shape.num_parts):\n",
    "        coords[i] = (shape.part(i).x, shape.part(i).y)\n",
    "\n",
    "    # return the list of (x, y)-coordinates\n",
    "    return coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rect_to_bb(rect):\n",
    "    # take a bounding predicted by dlib and convert it\n",
    "    # to the format (x, y, w, h) as we would normally do\n",
    "    # with OpenCV\n",
    "    x = rect.left()\n",
    "    y = rect.top()\n",
    "    w = rect.right() - x\n",
    "    h = rect.bottom() - y\n",
    "\n",
    "    # return a tuple of (x, y, w, h)\n",
    "    return (x, y, w, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dlib_shape(image):\n",
    "    # in this function we load the image, detect the landmarks of the face, and then return the image and the landmarks\n",
    "    # load the input image, resize it, and convert it to grayscale\n",
    "    resized_image = image.astype('uint8')\n",
    "\n",
    "    gray = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
    "    gray = gray.astype('uint8')\n",
    "\n",
    "    # detect faces in the grayscale image\n",
    "    rects = detector(gray, 1)\n",
    "    num_faces = len(rects)\n",
    "\n",
    "    if num_faces == 0:\n",
    "        return None, resized_image\n",
    "\n",
    "    face_areas = np.zeros((1, num_faces))\n",
    "    face_shapes = np.zeros((136, num_faces), dtype=np.int64) ## 5000 lines\n",
    "\n",
    "    # loop over the face detections\n",
    "    for (i, rect) in enumerate(rects):\n",
    "        # determine the facial landmarks for the face region, then\n",
    "        # convert the facial landmark (x, y)-coordinates to a NumPy\n",
    "        # array\n",
    "        temp_shape = predictor(gray, rect)\n",
    "        temp_shape = shape_to_np(temp_shape)\n",
    "\n",
    "        # convert dlib's rectangle to a OpenCV-style bounding box\n",
    "        # [i.e., (x, y, w, h)],\n",
    "        #   (x, y, w, h) = face_utils.rect_to_bb(rect)\n",
    "        (x, y, w, h) = rect_to_bb(rect)\n",
    "        face_shapes[:, i] = np.reshape(temp_shape, [136])\n",
    "        face_areas[0, i] = w * h\n",
    "    # find largest face and keep\n",
    "    dlibout = np.reshape(np.transpose(face_shapes[:, np.argmax(face_areas)]), [68, 2])\n",
    "\n",
    "    return dlibout, resized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_labels():\n",
    "    \n",
    "    image_paths = [os.path.join(images_dir, l) for l in os.listdir(images_dir)] # Cr√©er un path en incluant le nom de chaque mage ?\n",
    "    target_size = None\n",
    "    labels_file = open(os.path.join(basedir, labels_filename), 'r')\n",
    "    lines = labels_file.readlines()\n",
    "    gender_labels = {line.split(',')[0] : int(line.split(',')[2]) for line in lines[1:]}\n",
    "    if os.path.isdir(images_dir): # SI le Path existe \n",
    "        all_features = []\n",
    "        all_labels = []\n",
    "        for img_path in image_paths: # Pour chaque image \n",
    "            file_name= img_path.split('.')[1].split('/')[-1]\n",
    "\n",
    "            # load image\n",
    "            img = image.img_to_array(\n",
    "                image.load_img(img_path,\n",
    "                               target_size=target_size,\n",
    "                               interpolation='bicubic'))\n",
    "            features, _ = run_dlib_shape(img)\n",
    "            if features is not None:\n",
    "                all_features.append(features)\n",
    "                all_labels.append(gender_labels[file_name])\n",
    "\n",
    "    landmark_features = np.array(all_features)\n",
    "    gender_labels = (np.array(all_labels) + 1)/2 # simply converts the -1 into 0, so male=0 and female=1\n",
    "    return landmark_features, gender_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First we want to visualise the 68 labels on a scattered plot to ensure these functions are fully working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_1, feature_2 = extract_features_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Feature = feature_1[0]\n",
    "a,b = Feature.T\n",
    "plt.scatter(a,b)\n",
    "ax = plt.gca()\n",
    "ax.set_ylim(ax.get_ylim()[::-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVC:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will exctract the features from the pictures and the gender value from the csv file (with 0 instead of -1 for the male), using the extract_features_labels() function and separate these features into a training and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "\n",
    "    X, y = extract_features_labels()\n",
    "    Y = np.array([y, -(y - 1)]).T\n",
    "    tr_X = X[:3500]\n",
    "    tr_Y = Y[:3500]\n",
    "    te_X = X[3500:]\n",
    "    te_Y = Y[3500:]\n",
    "\n",
    "    return tr_X, tr_Y, te_X, te_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_X, tr_Y, te_X, te_Y= get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3500, 68, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1300, 68, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import sklearn functions for the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9292307692307692\n",
      "[1. 1. 0. ... 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "def img_SVM(training_images, training_labels, test_images, test_labels):\n",
    "    classifier = svm.SVC(kernel = 'linear')\n",
    "    classifier.fit(training_images, training_labels)\n",
    "    pred = classifier.predict(test_images)\n",
    "    print(\"Accuracy:\", accuracy_score(test_labels, pred))\n",
    "\n",
    "    print(pred)\n",
    "    return pred\n",
    "\n",
    "pred=img_SVM(tr_X.reshape((3500, 68*2)), list(zip(*tr_Y))[0], te_X.reshape((1300, 68*2)), list(zip(*te_Y))[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, using this model we get an accuracy of approximately 93%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Neighbors Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = extract_features_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4800, 136)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape\n",
    "X = X.reshape(4800,68*2)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test,y_train,y_test= train_test_split(X,y,test_size=0.3,random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set: 0.7  | test set: 0.3\n"
     ]
    }
   ],
   "source": [
    "print('train set: {}  | test set: {}'.format(round(((len(y_train)*1.0)/len(X)),3), round((len(y_test)*1.0)/len(X),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3360, 136), (3360,), array([1., 0., 0., ..., 1., 0., 0.]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to run a K-Neighbours Classifier using scikit learn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7736111111111111\n"
     ]
    }
   ],
   "source": [
    "def KNNClassifier(X_train, y_train, X_test,k):\n",
    "\n",
    "    #Create KNN object with a K coefficient\n",
    "    neigh = KNeighborsClassifier(n_neighbors=k)\n",
    "    neigh.fit(X_train, y_train) # Fit KNN model\n",
    "\n",
    "\n",
    "    Y_pred = neigh.predict(X_test)\n",
    "    return Y_pred\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Y_pred =KNNClassifier(X_train, y_train, X_test,4)\n",
    "  \n",
    "\n",
    "score=metrics.accuracy_score(y_test,Y_pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
